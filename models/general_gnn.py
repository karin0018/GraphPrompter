# A bit improved SingleLayerGeneralGNN.
# It supports an arbitrary order of different internal layers (the metagraph GNN, the background GNN, two supernode pooling-related layers)
# For possible layers, see models/layer_classes.py

import torch
import torch_geometric as pyg
import numpy as np
from models.layer_classes import MetagraphLayer, SupernodeAggrLayer, SupernodeToBgGraphLayer, BackgroundGNNLayer
from models.nodeformer import NodeFormer
from models.cache import LFUCacheE
from sklearn.manifold import TSNE
import sys
import random


class SingleLayerGeneralGNN(torch.nn.Module):

    def __init__(self,
                 layer_list,
                 initial_label_mlp=torch.nn.Identity(),
                 initial_input_mlp=torch.nn.Identity(),
                 final_label_mlp=torch.nn.Identity(),
                 final_input_mlp=torch.nn.Identity(),
                 params=None,
                 text_dropout=None,
                 num_prototypes4label = 1,
                 select_layers = None):
        super().__init__()
        self.layer_list = layer_list
        self.cos = torch.nn.CosineSimilarity(dim=1)
        self.initial_label_mlp = initial_label_mlp
        self.initial_input_mlp = initial_input_mlp  # project labels first (they might, for example,
        # have different shape at the begining than the embedding space)
        # TODO add select layers
        emb_dim = params["emb_dim"]
        self.select_layers = select_layers
        self.sigmoid = torch.nn.Sigmoid()
        self.final_label_mlp = final_label_mlp
        self.final_input_mlp = final_input_mlp
        self.learned_label_embedding = torch.nn.Embedding(
            1000, params["emb_dim"])

        if params is not None:
            self.params = params
        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        self.txt_dropout = text_dropout

        self.resample_number = 0
        self.resample_failed = 0
        self.sample_number = 0
        self.shots = params['n_shots']  # N shots!
        self.ways = params['n_way']  # n way classification!
        self.querys = params['n_query']
        self.temp = params['temp'] # choose temp-shots as prompts
        self.batch_size = params['batch_size']
        self.dataset_name = params['dataset']
        self.model_path = params['pretrained_model_run'][1:-10]
        # NOTE add prototypes for each label type:int
        self.num_prototypes4label = num_prototypes4label
        self.get_label_weight = torch.nn.Linear(num_prototypes4label*params["emb_dim"], params["emb_dim"])
 
        cap = params['cache_cap']*self.ways # *self.querys
        self.cache_strategy = LFUCacheE(capacity=cap,ways=self.ways)
        self.use_select = params['select']
        self.use_text_similarity = params['text_sim']
        self.select_lambda = params['select_lambda']

    def decode(self,
               input_x,
               label_x,
               metagraph_edge_index,
               edgelist_bipartite=False):
        '''
        :param input_x: As returned by the forward() method.
        :param label_x: As returned by the forward() method.
        :param edgelist_bipartite: Whether edgelist is bipartite, i.e. both left and right side are numbered from 0.
        :return:
        '''
        if edgelist_bipartite:
            ind0 = metagraph_edge_index[0, :]
            ind1 = metagraph_edge_index[1, :]
            decoded_logits = (self.cos(input_x[ind0], label_x[ind1]) + 1) / 2
            return decoded_logits
        if self.num_prototypes4label > 1:
            label_x_multi_prototype = torch.cat(torch.chunk(label_x, self.num_prototypes4label, dim=0), dim=1)
            label_x= self.get_label_weight(label_x_multi_prototype)
            origin_index_len = metagraph_edge_index.shape[1] // 2
            x = torch.cat((input_x, label_x))
            ind0 = metagraph_edge_index[0, :origin_index_len]
            ind1 = metagraph_edge_index[1, :origin_index_len]
            decoded_logits = self.cos(x[ind0], x[ind1]) * self.logit_scale.exp()
            return decoded_logits

        x = torch.cat((input_x, label_x))
        ind0 = metagraph_edge_index[0, :]
        ind1 = metagraph_edge_index[1, :]
        decoded_logits = self.cos(x[ind0], x[ind1]) * self.logit_scale.exp()
        return decoded_logits

    def forward_metagraph(self, module, supernode_x, label_x,
                          metagraph_edge_index, metagraph_edge_attr,
                          query_set_mask, input_seqs, query_seqs,
                          query_seqs_gt):
        '''
        Forward pass on the graph embedding <-> task bipartite metagraph.
        supernode_x: output from forward1 - matrix of pooled (sub)graph embeddings.
        label_x: matrix of label embeddings - either generated by BERT (previous step) or output from previous
                 SingleLayerGeneralGNN
        metagraph_edge_index: edge_index of a directed bipartite graph mapping class embedding index to class idx.
                              Both left and right side start counting node idx from 0!
        metagraph_edge_attr: edge_attr of the metagraph
        :return: Updated pooled embeddings and task embeddings of the bipartite graph.
        '''
        x = torch.cat((supernode_x, label_x))
        if not self.params['zero_shot']:

            # DEBUG
            # print('running metagraph layers ....')
            # print(
            #     f'supernode_x.shape = {supernode_x.shape}, metagraph_edge_index.shape={metagraph_edge_index.shape}, metagraph_edge_attr={metagraph_edge_attr.shape}, query_set_mask={query_set_mask.shape}'
            # )
            x = module(x=x,
                       edge_index=metagraph_edge_index,
                       edge_attr=metagraph_edge_attr,
                       query_mask=query_set_mask,
                       start_right=supernode_x.shape[0],
                       input_seqs=input_seqs,
                       query_seqs=query_seqs,
                       query_seqs_gt=query_seqs_gt)
        input_x_mg = x[:supernode_x.shape[0]]
        label_x_mg = x[supernode_x.shape[0]:]

        assert len(input_x_mg) == len(supernode_x)
        assert len(label_x_mg) == len(label_x)

        return input_x_mg, label_x_mg

    def forward_metaNodeFormerLayer(self,
                                    module,
                                    supernode_x,
                                    label_x,
                                    metagraph_edge_index,
                                    query_set_mask,
                                    tau=1.0):
        x = torch.cat((supernode_x, label_x))

        if not self.params['zero_shot']:
            x, link_loss_ = module(x,
                                   metagraph_edge_index,
                                   query_mask=query_set_mask,
                                   tau=tau)
            x = x.squeeze(0)
        input_x_mg = x[:supernode_x.shape[0]]
        label_x_mg = x[supernode_x.shape[0]:]

        assert len(input_x_mg) == len(supernode_x)
        assert len(label_x_mg) == len(label_x)

        return input_x_mg, label_x_mg, link_loss_
    
    def gumbel_softmax_sample(self, log_alpha, temperature, istrain):
        if istrain:
            debug_var = 1e-5
            bias = 0.0
            r = bias + torch.rand(log_alpha.size(), dtype=log_alpha.dtype) * (1.0 - 2 * debug_var) + debug_var
            gate_inputs = torch.log(r) - torch.log(1.0 - r)
            gate_inputs = (gate_inputs + log_alpha) / temperature
            gate_inputs = self.sigmoid(gate_inputs)
        else:
            gate_inputs = self.sigmoid(log_alpha)

        stretched_values = gate_inputs * (self.zeta - self.gamma) + self.gamma
        clipped = torch.clamp(stretched_values, 1e-20, 1.0)
        return clipped

    def forward(self,
                graph,
                x_label,
                y_true_matrix,
                metagraph_edge_index,
                metagraph_edge_attr,
                query_set_mask,
                input_seqs=None,
                query_seqs=None,
                query_seqs_gt=None,
                task_mask=None,
                knn=False,
                cache=False,
                add_edges=False,
                add_norm=False,
                shots=3,
                querys=4,
                tau=1.0,
                temperature=1.0,
                training=False):
        '''
        Params as returned by the batching function.
        # task_mask: Not actually needed here, but is passed here from the dataloader batch output..
        :return: y_true_matrix, y_pred_matrix (for the query set only!)
        '''
        supernode_idx = graph.supernode + graph.ptr[:-1]
        #center_nodes = torch.zeros([graph.x.shape[0], 1]).to(graph.x.device)
        #center_nodes[graph.ptr[:-1]] = 1
        #graph.x = self.initial_input_mlp(torch.concat([graph.x, center_nodes], dim = 1))
        graph.x = self.initial_input_mlp(graph.x)
        self.sample_number += 1

        if self.txt_dropout is not None:
            graph.x = self.txt_dropout(graph.x)
            if "edge_attr" in graph and graph.edge_attr is not None:
                graph.edge_attr = self.txt_dropout(graph.edge_attr)

        x_orig = graph.x.clone()

        x_label = self.initial_label_mlp(x_label)
        if self.params["ignore_label_embeddings"]:
            # x_label = torch.zeros_like(x_label).float()  # to make sure no language information is passed through the model
            # x_label = torch.nn.ReLU()(torch.normal(0, 6,x_label.shape).to(x_label.device))
            x_label = self.learned_label_embedding(
                torch.arange(x_label.shape[0]).to(x_label.device))
        if self.params["zero_label_embeddings"]:
            x_label = torch.zeros_like(x_label).float()
        '''
        # temporary code for debugging
        bg_gnn = self.layer_list[0]
        supernode_aggr = self.layer_list[1]
        meta_gnn = self.layer_list[2]

        node_embs = bg_gnn(graph.x, graph.edge_index, graph.edge_attr)
        supernode_x = supernode_aggr(node_embs, graph.edge_index_supernode, supernode_idx)
        x_input, x_label = self.forward_metagraph(meta_gnn, supernode_x, x_label, metagraph_edge_index, metagraph_edge_attr)
        '''

        x_input = torch.zeros(
            (len(supernode_idx), x_label.size(1))).float().to(x_label.device)
        x_input_orig = x_orig[supernode_idx]
        
        qry_idx_origin = torch.where(
            query_set_mask.reshape(-1, y_true_matrix.shape[1])[:, 0] == 1)[0]
        link_loss_ = None
        # n_ways = qry_idx_origin.shape[0] // querys
        n_ways = self.ways

        for module in self.layer_list:
            if isinstance(module, NodeFormer):
                if x_input is None:
                    raise Exception(
                        'MetagraphLayer must be preceded by a layer that produces supernode embeddings!'
                    )
                x_input, new_x_label, link_loss_ = self.forward_metaNodeFormerLayer(
                    module, x_input, x_label, metagraph_edge_index,
                    query_set_mask, tau)
                if self.params["skip_path"]:
                    x_label = x_label + new_x_label
                else:
                    x_label = new_x_label

            elif isinstance(module, MetagraphLayer):
                if x_input is None:
                    raise Exception(
                        'MetagraphLayer must be preceded by a layer that produces supernode embeddings!'
                    )
                x_input, new_x_label = self.forward_metagraph(
                    module, x_input, x_label, metagraph_edge_index,
                    metagraph_edge_attr, query_set_mask, input_seqs, query_seqs,
                    query_seqs_gt)
                if self.params["skip_path"]:
                    x_label = x_label + new_x_label
                else:
                    x_label = new_x_label
            elif isinstance(module, SupernodeAggrLayer):
                x_input = module.forward(graph.x, graph.edge_index_supernode,
                                         supernode_idx, graph.batch)
                #  This is needed to allow backpropagation - but it is perhaps not the best solution:
                graph.x = graph.x.clone()
                graph.x[supernode_idx] = x_input
                
                if self.use_select:
                    x_weights = self.select_layers(x_input)
                    x_weights = self.sigmoid(x_weights)

                    
                # divided group by label and calculate the similarity of prompt supernode and query node
                if knn:
                    x_input_new = None
                    x_weight_new = None
                    new_index = torch.tensor([]).to(x_input.device)

                    qry_idx_origin = torch.where(
                        query_set_mask.reshape(-1, y_true_matrix.shape[1])[:, 0]
                        == 1)[0]
                    prompt_idx_origin = torch.where(
                        query_set_mask.reshape(-1, y_true_matrix.shape[1])[:, 0]
                        == 0)[0]
                    x_prompts = x_input[prompt_idx_origin]
                    
                    x_prompts_weights = x_weights[prompt_idx_origin] if self.use_select else None
                    x_prompts_orig = x_input_orig[prompt_idx_origin] if self.use_text_similarity else None
                    # pid2xid = {i:prompt_idx_origin[i] for i in range(prompt_idx_origin.shape[0])}
                    pid2votes = [0 for _ in range(prompt_idx_origin.shape[0])]


                    # TODO cache initial 
                    # if cache and self.cache_strategy.size == 0:
                    #     y_prompt = y_true_matrix[prompt_idx_origin, :]
                    #     random_indices = random.sample(range(x_prompts.shape[0]),self.cache_strategy.ncap)
                    #     initial_prompt = x_prompts[random_indices].to('cpu')
                    #     initial_value = y_prompt[random_indices, :].to('cpu')
                    #     initial_value = torch.argmax(initial_value,dim=1)
                    #     for i in range(self.cache_strategy.ncap):
                    #         self.cache_strategy.put(initial_prompt[i], initial_value[i].item())

                    n_k = (shots//10)*self.temp if (shots//10) > 1 else self.temp
                    i=0
                    for qry in qry_idx_origin:
                        similarity = self.cos(x_input[qry], x_prompts)
                        if self.use_text_similarity:
                            text_similarity = self.cos(x_input_orig[qry], x_prompts_orig) 
                            similarity += text_similarity * self.select_lambda
                        if self.use_select:
                            similarity += x_prompts_weights.T.squeeze(0) * x_weights[qry]
                        top_k_value, top_k_indices = torch.topk(similarity, k=n_k)

                        for id in range(n_k):
                            pid2votes[top_k_indices[id].item()] += top_k_value[id].item()
                            pid2votes[top_k_indices[id].item()] += x_weights[top_k_indices[id].item()] * self.select_lambda if self.use_select else 0

                    for i in range(n_ways):
                        index2vote = torch.tensor(pid2votes[i * shots:(i + 1) *
                                                            shots])
                        top_k_value, top_k_indices = torch.topk(index2vote, k=n_k)
                        prompt_index = i * shots + top_k_indices
                        # add prompt to input
                        new_input = torch.index_select(x_prompts, dim=0, index=prompt_index.to(x_input.device))
                        new_input_weight = torch.index_select(x_prompts_weights, dim=0, index=prompt_index.to(x_input.device)) if self.use_select else None
                        # add query to input
                        new_input = torch.cat(
                            (new_input,
                             x_input[qry_idx_origin[i * querys:(i + 1) *
                                                    querys]]),
                            dim=0)
                        new_input_weight = torch.cat(
                            (new_input_weight,
                             x_weights[qry_idx_origin[i * querys:(i + 1) *
                                                    querys]]),
                            dim=0) if self.use_select else None

                        new_index = torch.cat(
                            (new_index, prompt_idx_origin[prompt_index].to(
                                x_input.device)))
                        new_index = torch.cat(
                            (new_index,
                             qry_idx_origin[i * querys:(i + 1) * querys].to(
                                 x_input.device)))

                        if i == 0:
                            x_input_new = new_input
                            x_weight_new = new_input_weight
                        else:
                            x_input_new = torch.cat((x_input_new, new_input),
                                                    dim=0)
                            x_weight_new = torch.cat((x_weight_new, new_input_weight),
                                                    dim=0) if self.use_select else None

                    new_index = new_index.to(torch.int)
                    x_input = x_input_new.to(x_input.device)
                    x_weights = x_weight_new.to(x_input.device) if self.use_select else None
                    
                    # query_set_mask
                    query_set_mask_temp = query_set_mask.reshape(
                        y_true_matrix.shape)
                    query_set_mask_temp = query_set_mask_temp[new_index, :]
                    query_set_mask = query_set_mask_temp.view(-1)
                    # metagraph_edge_index
                    # !!! Attention: you must set the batchsize=1(-bs 1), or you will get an error here :)
                    metagraph_edge_index = metagraph_edge_index[:, :len(
                        query_set_mask)]
                    metagraph_edge_index[1] = torch.tensor(
                        range(
                            metagraph_edge_index[0][-1] + 1,
                            metagraph_edge_index[0][-1] + 1 + x_label.shape[0])
                    ).repeat(metagraph_edge_index[0][-1] + 1).to(x_input.device)

                    # metagraph_edge_attr
                    metagraph_edge_attr_temp = metagraph_edge_attr.reshape(
                        y_true_matrix.shape[0], y_true_matrix.shape[1], -1)
                    metagraph_edge_attr_temp = metagraph_edge_attr_temp[
                        new_index, :]
                    metagraph_edge_attr = metagraph_edge_attr_temp.reshape(
                        -1, 2)

                    y_true_matrix = y_true_matrix[new_index, :]

                    shots = n_k
                
                if self.use_select:
                    x_input = x_input * x_weights
                    
                if cache and self.cache_strategy.size > 0:
                    device = x_input.device
                    cache_prompts_dict = self.cache_strategy.get_all()

                    cache_prompts_group = torch.tensor([]).to(device)
                    new_input_prompts_cache = torch.tensor([]).to(device)
                    query_set_mask_prompts_cache = torch.tensor([]).to(device)
                    metagraph_edge_index_cache = torch.tensor([]).to(device)
                    metagraph_edge_attr_cache = torch.tensor([]).to(device)
                    y_true_matrix_prompts_cache = torch.tensor([]).to(device)

                    for i in range(n_ways):

                        cache_prompts_group = torch.cat((cache_prompts_group, cache_prompts_dict[i].to(device)))

                        new_input = x_input[(shots+querys)*i:(shots+querys)*(i+1)]
                        new_input = torch.cat((cache_prompts_dict[i].to(device), new_input))
                        new_input_prompts_cache = torch.cat((new_input_prompts_cache, new_input))

                        new_query_set = query_set_mask[(shots+querys)*i*n_ways : (shots+querys)*(i+1)*n_ways]
                        false_mask = torch.tensor([False]).repeat(cache_prompts_dict[i].shape[0]*n_ways)
                        new_query_set = torch.cat((false_mask.to(device) , new_query_set))
                        query_set_mask_prompts_cache = torch.cat((query_set_mask_prompts_cache, new_query_set))

                        new_edge_attr = metagraph_edge_attr[(shots+querys)*i*n_ways : (shots+querys)*(i+1)*n_ways]
                        negs = torch.tensor([0.0,-1.0]).repeat(n_ways,1)
                        negs[i][1] = 1.0
                        new_edge_attr = torch.cat((negs.repeat(cache_prompts_dict[i].shape[0],1).to(device), new_edge_attr))
                        metagraph_edge_attr_cache = torch.cat((metagraph_edge_attr_cache, new_edge_attr))

                        zeros = torch.zeros(n_ways).to(int)
                        zeros[i] = 1
                        one_hot = zeros.repeat(cache_prompts_dict[i].shape[0],1).to(device)
                        new_y_true = y_true_matrix[(shots+querys)*i:(shots+querys)*(i+1)]
                        new_y_true = torch.cat((one_hot, new_y_true))
                        y_true_matrix_prompts_cache = torch.cat((y_true_matrix_prompts_cache,new_y_true))



                    x_input = new_input_prompts_cache
                    query_set_mask = query_set_mask_prompts_cache
                    y_true_matrix = y_true_matrix_prompts_cache
                    metagraph_edge_attr = metagraph_edge_attr_cache

                    edge_index_0 = torch.arange(metagraph_edge_index[0][-1] + 1,
                                                    metagraph_edge_index[0][-1] + 1 + self.cache_strategy.size).repeat_interleave(n_ways).to(device)

                    edge_index_0 = torch.cat((metagraph_edge_index[0],edge_index_0))
                    edge_index_1 = torch.tensor(
                        range(
                            edge_index_0[-1] + 1,
                            edge_index_0[-1] + 1 + n_ways)
                    ).repeat(edge_index_0[-1] + 1).to(device)

                    metagraph_edge_index = torch.cat((edge_index_0.unsqueeze(0), edge_index_1.unsqueeze(0)))  # shape [2 N]

                    qry_idx_origin = torch.where(
                        query_set_mask.reshape(-1, y_true_matrix.shape[1])[:, 0]
                        == 1)[0]
                    for qry in qry_idx_origin:
                        similarity = self.cos(x_input[qry], cache_prompts_group)
                        # similarity = torch.softmax(torch.matmul(x_input[qry], x_prompts.T),dim=0)
                        top_k_value, top_k_indices = torch.topk(similarity,k=1)
                        for id in top_k_indices:
                            self.cache_strategy.get(cache_prompts_group[id.item()].to('cpu'))


                if self.num_prototypes4label > 1:
                    add_edge_index_0 = torch.tensor([]).to(x_input.device)
                    add_edge_index_1 = torch.tensor([]).to(x_input.device)
                    add_edge_attr = torch.tensor([]).to(x_input.device)
                    add_query_mask = torch.tensor([]).to(x_input.device)
                    add_x_label = torch.tensor([]).to(x_label.device)

                    for _ in range(self.num_prototypes4label-1):

                        for bs in range(self.batch_size):
                            edge_index = torch.arange(metagraph_edge_index[1][-1] + 1 + n_ways*bs, metagraph_edge_index[1][-1] + 1 + n_ways*(bs+1)).repeat(n_ways*(querys+shots)).to(x_input.device)
                            add_edge_index_1 = torch.cat((add_edge_index_1,edge_index))

                        add_edge_index_0 = torch.cat((add_edge_index_0, metagraph_edge_index[0].clone()))
                        add_edge_attr = torch.cat((add_edge_attr,
                                                metagraph_edge_attr.clone()))
                        add_query_mask = torch.cat(
                            (add_query_mask, query_set_mask.clone()))
                        add_x_label = torch.cat((add_x_label,x_label))
                    add_edge_index = torch.cat((add_edge_index_0.unsqueeze(0), add_edge_index_1.unsqueeze(0))) # [N 2]
                    add_edge_index = add_edge_index.to(torch.int)
                    metagraph_edge_index = torch.cat((metagraph_edge_index, add_edge_index), dim=1) # [2 N]
                    metagraph_edge_attr = torch.cat((metagraph_edge_attr, add_edge_attr)) # [N 2]
                    query_set_mask = torch.cat((query_set_mask, add_query_mask)) # [N]
                    x_label = torch.cat((x_label,add_x_label)) # [M D]

                if add_edges:
                    edge_index = []
                    similarities = torch.mm(x_input,x_input.t())
                    top_k_values, top_k_indices = torch.topk(similarities, dim=1, k=(n_ways+querys))
                    for i in range(top_k_indices.shape[0]):
                        for j in range(top_k_indices.shape[1]):
                            # for node i, add edges to i-j, which j is the k-nearest neighbor node for i
                            edge_index.append([i,top_k_indices[i][j].item()])
                    edge_index = torch.tensor(edge_index).to(x_input.device)  # shape=[N,2]
                    edge_attr = torch.zeros(edge_index.shape).to(x_input.device)  # shape=[N,2]
                    false_attr = torch.tensor([
                        False for i in range(edge_attr.shape[0])
                    ]).to(x_input.device)
                    metagraph_edge_index = torch.cat((metagraph_edge_index.t(),edge_index),dim=0).t() # shape = [2,N]
                    metagraph_edge_attr = torch.cat((metagraph_edge_attr,edge_attr),dim=0) # shape = [N,2]
                    query_set_mask = torch.cat((query_set_mask,false_attr)) # shape = [N]
                    self.edge_index_len = edge_index.shape[0]

            elif isinstance(module, SupernodeToBgGraphLayer):
                if x_input is None:
                    raise Exception(
                        "SupernodeToBgGraphLayer must be preceded by MetagraphLayer!"
                    )
                new_graph_x = module.forward(graph.x, x_input,
                                             graph.edge_index_supernode,
                                             supernode_idx, graph.batch)
                if self.params["skip_path"]:
                    graph.x = graph.x + new_graph_x
                else:
                    graph.x = new_graph_x
            elif isinstance(module, BackgroundGNNLayer):
                new_graph_x = module.forward(
                    x_orig, graph.x, graph.edge_index.long(),
                    graph.edge_attr if "edge_attr" in graph else None,
                    graph.edge_index_supernode, graph.ptr[:-1], graph.batch, temperature=temperature, training=training, edge_fliter_feat=graph.edge_fliter_feat if "edge_fliter_feat" in graph else None,)
                if self.params[
                        "skip_path"] and new_graph_x.shape == graph.x.shape:
                    graph.x = graph.x + new_graph_x
                else:
                    graph.x = new_graph_x
            else:
                raise ValueError('Unknown layer type: {}'.format(type(module)))

        x_input = self.final_input_mlp(x_input)
        # NOTE add normalize
        if add_norm:
            print('use add norm')
            x_input = torch.nn.functional.normalize(x_input)
        x_label = self.final_label_mlp(x_label)

        if add_edges:
            metagraph_edge_index =  metagraph_edge_index[:, :-self.edge_index_len]
            query_set_mask = query_set_mask[:-self.edge_index_len]


        y_pred_matrix = self.decode(x_input,
                                    x_label,
                                    metagraph_edge_index,
                                    edgelist_bipartite=False).reshape(
                                    y_true_matrix.shape)

        if self.num_prototypes4label > 1:
            origin_index_len = metagraph_edge_index.shape[1] // 2
            query_set_mask = query_set_mask[:origin_index_len]

        qry_idx = torch.where(query_set_mask.reshape(-1, y_true_matrix.shape[1])[:,0] == 1)[0]


        # NOTE add cache_strategy and knn augmentation
        if cache:
            # y_true_class = torch.argmax(y_true_matrix[qry_idx, :], dim=1).to('cpu')
            y_pred_class = torch.argmax(y_pred_matrix[qry_idx, :], dim=1).to('cpu')

            ypred_group = y_pred_matrix[qry_idx, :].to('cpu')
            qry_embeddings = x_input[qry_idx,:].to('cpu')

            for i in range(self.ways):
                indices = torch.where(y_pred_class==i)[0]
                if len(indices) == 0:
                    continue
                label_qrys = qry_embeddings[indices]
                label_y = ypred_group[indices]
                max_index = torch.argmax(label_y)
                max_row = max_index // label_y.shape[1]
                self.cache_strategy.put(label_qrys[max_row.item()],i)
                


        # NOTE modified the return value to support supervised contrastive loss function
        # return y_true_matrix[qry_idx, :], y_pred_matrix[qry_idx, :], graph
        return y_true_matrix, y_pred_matrix, x_input, qry_idx, graph, link_loss_
